{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Model Tuning Notebook\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Train face recognition models with reference photos\n",
    "2. Analyze performance on labeled photo datasets\n",
    "3. Fine-tune parameters (tolerance, model, encoding_model, num_jitters)\n",
    "4. Track experiments and compare results\n",
    "\n",
    "## Prerequisites\n",
    "- Install dependencies: `pip install -r requirements.txt`\n",
    "- Install face_recognition: See `docs/FACE_RECOGNITION_LOCAL_SETUP.md`\n",
    "- Place reference photos in `reference_photos/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Face recognition\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import yaml\n",
    "\n",
    "# Add scripts directory to path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"scripts\"))\n",
    "\n",
    "from scripts.face_recognizer.providers.local_provider import LocalFaceRecognitionProvider\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Face recognition version: {face_recognition.__version__ if hasattr(face_recognition, '__version__') else 'unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "CONFIG_PATH = PROJECT_ROOT / \"config\" / \"config.yaml\"\n",
    "\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Loaded configuration from config.yaml\")\n",
    "else:\n",
    "    # Default configuration\n",
    "    config = {\n",
    "        'face_recognition': {\n",
    "            'reference_photos_dir': './reference_photos',\n",
    "            'tolerance': 0.6,\n",
    "            'local': {\n",
    "                'model': 'hog',\n",
    "                'encoding_model': 'large',\n",
    "                'training': {'num_jitters': 50},\n",
    "                'recognition': {'num_jitters': 1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(\"Using default configuration\")\n",
    "\n",
    "# Extract face recognition settings\n",
    "face_config = config.get('face_recognition', {})\n",
    "local_config = face_config.get('local', {})\n",
    "\n",
    "print(f\"\\nCurrent settings:\")\n",
    "print(f\"  Reference photos dir: {face_config.get('reference_photos_dir', './reference_photos')}\")\n",
    "print(f\"  Tolerance: {face_config.get('tolerance', 0.6)}\")\n",
    "print(f\"  Detection model: {local_config.get('model', 'hog')}\")\n",
    "print(f\"  Encoding model: {local_config.get('encoding_model', 'large')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Reference Photos & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find reference photos\n",
    "REFERENCE_DIR = PROJECT_ROOT / face_config.get('reference_photos_dir', './reference_photos').lstrip('./')\n",
    "IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.heic']\n",
    "\n",
    "def get_reference_photos(reference_dir, extensions):\n",
    "    \"\"\"Find all reference photos in directory.\"\"\"\n",
    "    photos = []\n",
    "    for ext in extensions:\n",
    "        photos.extend(glob.glob(str(reference_dir / f\"*{ext}\")))\n",
    "        photos.extend(glob.glob(str(reference_dir / f\"*{ext.upper()}\")))\n",
    "    # Filter out system files\n",
    "    photos = [p for p in photos if not os.path.basename(p).startswith('.')]\n",
    "    return sorted(list(set(photos)))\n",
    "\n",
    "reference_photos = get_reference_photos(REFERENCE_DIR, IMAGE_EXTENSIONS)\n",
    "print(f\"Found {len(reference_photos)} reference photos in {REFERENCE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reference photos in a grid\n",
    "def display_photos_grid(photo_paths, max_photos=12, figsize=(15, 10)):\n",
    "    \"\"\"Display photos in a grid layout.\"\"\"\n",
    "    n_photos = min(len(photo_paths), max_photos)\n",
    "    if n_photos == 0:\n",
    "        print(\"No photos to display\")\n",
    "        return\n",
    "    \n",
    "    cols = min(4, n_photos)\n",
    "    rows = (n_photos + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for idx, photo_path in enumerate(photo_paths[:max_photos]):\n",
    "        row, col = idx // cols, idx % cols\n",
    "        try:\n",
    "            img = Image.open(photo_path)\n",
    "            img.thumbnail((300, 300))\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(os.path.basename(photo_path)[:20], fontsize=8)\n",
    "        except Exception as e:\n",
    "            axes[row, col].text(0.5, 0.5, f\"Error: {e}\", ha='center', va='center')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_photos, rows * cols):\n",
    "        row, col = idx // cols, idx % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Reference Photos ({n_photos} shown)\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "display_photos_grid(reference_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(reference_photos, model='hog', encoding_model='large', num_jitters=50):\n",
    "    \"\"\"Train face recognition model with given parameters.\"\"\"\n",
    "    provider_config = {\n",
    "        'model': model,\n",
    "        'encoding_model': encoding_model,\n",
    "        'num_jitters': num_jitters,\n",
    "        'tolerance': 0.6  # Default, will be overridden during matching\n",
    "    }\n",
    "    \n",
    "    provider = LocalFaceRecognitionProvider(provider_config)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    num_faces = provider.load_reference_photos(reference_photos)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return provider, num_faces, training_time\n",
    "\n",
    "# Train with default parameters\n",
    "print(\"Training model with default parameters...\")\n",
    "provider, num_faces, training_time = train_model(\n",
    "    reference_photos,\n",
    "    model=local_config.get('model', 'hog'),\n",
    "    encoding_model=local_config.get('encoding_model', 'large'),\n",
    "    num_jitters=local_config.get('training', {}).get('num_jitters', 50)\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(f\"  Photos processed: {len(reference_photos)}\")\n",
    "print(f\"  Faces detected: {num_faces}\")\n",
    "print(f\"  Success rate: {100.0 * num_faces / len(reference_photos):.1f}%\")\n",
    "print(f\"  Training time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Test Dataset Creation Helper\n",
    "\n",
    "Create a labeled test dataset by organizing photos into folders:\n",
    "```\n",
    "notebooks/data/test_photos/\n",
    "    match/       # Photos containing the target person\n",
    "    no_match/    # Photos NOT containing the target person\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset paths\n",
    "TEST_DATA_DIR = Path(os.getcwd()) / \"data\" / \"test_photos\"\n",
    "MATCH_DIR = TEST_DATA_DIR / \"match\"\n",
    "NO_MATCH_DIR = TEST_DATA_DIR / \"no_match\"\n",
    "\n",
    "def create_test_dataset_folders():\n",
    "    \"\"\"Create the folder structure for test dataset.\"\"\"\n",
    "    MATCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    NO_MATCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created test dataset folders:\")\n",
    "    print(f\"  Match folder: {MATCH_DIR}\")\n",
    "    print(f\"  No-match folder: {NO_MATCH_DIR}\")\n",
    "    print(f\"\\nPlace your test photos in these folders and re-run the next cell.\")\n",
    "\n",
    "def load_test_dataset():\n",
    "    \"\"\"Load test dataset from folder structure.\"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Load match photos\n",
    "    if MATCH_DIR.exists():\n",
    "        for ext in IMAGE_EXTENSIONS:\n",
    "            for photo in glob.glob(str(MATCH_DIR / f\"*{ext}\")):\n",
    "                if not os.path.basename(photo).startswith('.'):\n",
    "                    dataset.append({'file_path': photo, 'ground_truth': 'match'})\n",
    "            for photo in glob.glob(str(MATCH_DIR / f\"*{ext.upper()}\")):\n",
    "                if not os.path.basename(photo).startswith('.'):\n",
    "                    dataset.append({'file_path': photo, 'ground_truth': 'match'})\n",
    "    \n",
    "    # Load no-match photos\n",
    "    if NO_MATCH_DIR.exists():\n",
    "        for ext in IMAGE_EXTENSIONS:\n",
    "            for photo in glob.glob(str(NO_MATCH_DIR / f\"*{ext}\")):\n",
    "                if not os.path.basename(photo).startswith('.'):\n",
    "                    dataset.append({'file_path': photo, 'ground_truth': 'no_match'})\n",
    "            for photo in glob.glob(str(NO_MATCH_DIR / f\"*{ext.upper()}\")):\n",
    "                if not os.path.basename(photo).startswith('.'):\n",
    "                    dataset.append({'file_path': photo, 'ground_truth': 'no_match'})\n",
    "    \n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "# Create folders if they don't exist\n",
    "if not TEST_DATA_DIR.exists():\n",
    "    create_test_dataset_folders()\n",
    "else:\n",
    "    print(f\"Test dataset folder exists: {TEST_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display test dataset\n",
    "test_df = load_test_dataset()\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    print(\"No test photos found!\")\n",
    "    print(f\"\\nPlease add photos to:\")\n",
    "    print(f\"  {MATCH_DIR}  (photos with the target person)\")\n",
    "    print(f\"  {NO_MATCH_DIR}  (photos without the target person)\")\n",
    "else:\n",
    "    print(f\"Loaded {len(test_df)} test photos:\")\n",
    "    print(test_df['ground_truth'].value_counts())\n",
    "    print(f\"\\nDataset preview:\")\n",
    "    display(test_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(provider, test_df, tolerance=0.6):\n",
    "    \"\"\"Evaluate model on test dataset.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        ground_truth = row['ground_truth']\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load image\n",
    "            image = face_recognition.load_image_file(file_path)\n",
    "            image_bytes = open(file_path, 'rb').read()\n",
    "            \n",
    "            # Find matches\n",
    "            matches, total_faces = provider.find_matches_in_image(\n",
    "                image_bytes, \n",
    "                source=file_path,\n",
    "                tolerance=tolerance\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Determine prediction\n",
    "            is_match = len(matches) > 0\n",
    "            confidence = matches[0].get('confidence', 0.0) if matches else 0.0\n",
    "            \n",
    "            results.append({\n",
    "                'file_path': file_path,\n",
    "                'ground_truth': ground_truth,\n",
    "                'prediction': 'match' if is_match else 'no_match',\n",
    "                'is_correct': (ground_truth == 'match') == is_match,\n",
    "                'confidence': confidence,\n",
    "                'faces_detected': total_faces,\n",
    "                'matches_found': len(matches),\n",
    "                'processing_time': processing_time,\n",
    "                'error': None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'file_path': file_path,\n",
    "                'ground_truth': ground_truth,\n",
    "                'prediction': 'error',\n",
    "                'is_correct': False,\n",
    "                'confidence': 0.0,\n",
    "                'faces_detected': 0,\n",
    "                'matches_found': 0,\n",
    "                'processing_time': 0.0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation if we have test data\n",
    "if len(test_df) > 0:\n",
    "    print(f\"Evaluating model on {len(test_df)} test photos...\")\n",
    "    results_df = evaluate_model(provider, test_df, tolerance=face_config.get('tolerance', 0.6))\n",
    "    \n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"  Correct: {results_df['is_correct'].sum()} / {len(results_df)}\")\n",
    "    print(f\"  Errors: {(results_df['error'].notna()).sum()}\")\n",
    "    display(results_df.head(10))\n",
    "else:\n",
    "    print(\"No test data available. Please add test photos first.\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Metrics & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results_df):\n",
    "    \"\"\"Calculate classification metrics.\"\"\"\n",
    "    # Filter out errors\n",
    "    valid_df = results_df[results_df['error'].isna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    y_true = (valid_df['ground_truth'] == 'match').astype(int)\n",
    "    y_pred = (valid_df['prediction'] == 'match').astype(int)\n",
    "    y_scores = valid_df['confidence']\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'total_samples': len(valid_df),\n",
    "        'true_positives': ((y_true == 1) & (y_pred == 1)).sum(),\n",
    "        'true_negatives': ((y_true == 0) & (y_pred == 0)).sum(),\n",
    "        'false_positives': ((y_true == 0) & (y_pred == 1)).sum(),\n",
    "        'false_negatives': ((y_true == 1) & (y_pred == 0)).sum(),\n",
    "        'avg_processing_time': valid_df['processing_time'].mean()\n",
    "    }\n",
    "    \n",
    "    # Calculate AUC if we have both classes\n",
    "    if len(y_true.unique()) > 1 and y_scores.sum() > 0:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        metrics['auc'] = auc(fpr, tpr)\n",
    "    else:\n",
    "        metrics['auc'] = None\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    metrics = calculate_metrics(results_df)\n",
    "    if metrics:\n",
    "        print(\"Classification Metrics:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.3f}\")\n",
    "        print(f\"  F1 Score:  {metrics['f1_score']:.3f}\")\n",
    "        if metrics['auc']:\n",
    "            print(f\"  AUC:       {metrics['auc']:.3f}\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(f\"  TP: {metrics['true_positives']}  FP: {metrics['false_positives']}\")\n",
    "        print(f\"  FN: {metrics['false_negatives']}  TN: {metrics['true_negatives']}\")\n",
    "        print(f\"\\nAvg processing time: {metrics['avg_processing_time']:.3f}s per image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results_df):\n",
    "    \"\"\"Create visualization plots for evaluation results.\"\"\"\n",
    "    valid_df = results_df[results_df['error'].isna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        print(\"No valid results to plot\")\n",
    "        return\n",
    "    \n",
    "    y_true = (valid_df['ground_truth'] == 'match').astype(int)\n",
    "    y_pred = (valid_df['prediction'] == 'match').astype(int)\n",
    "    y_scores = valid_df['confidence']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "                xticklabels=['No Match', 'Match'], yticklabels=['No Match', 'Match'])\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    axes[0, 0].set_ylabel('Actual')\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    if len(y_true.unique()) > 1 and y_scores.sum() > 0:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[0, 1].plot(fpr, tpr, 'b-', label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "        axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        axes[0, 1].set_xlabel('False Positive Rate')\n",
    "        axes[0, 1].set_ylabel('True Positive Rate')\n",
    "        axes[0, 1].set_title('ROC Curve')\n",
    "        axes[0, 1].legend()\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Insufficient data for ROC curve', \n",
    "                        ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('ROC Curve')\n",
    "    \n",
    "    # 3. Confidence Distribution\n",
    "    match_conf = valid_df[valid_df['ground_truth'] == 'match']['confidence']\n",
    "    no_match_conf = valid_df[valid_df['ground_truth'] == 'no_match']['confidence']\n",
    "    \n",
    "    if len(match_conf) > 0:\n",
    "        axes[1, 0].hist(match_conf, bins=20, alpha=0.7, label='Match', color='green')\n",
    "    if len(no_match_conf) > 0:\n",
    "        axes[1, 0].hist(no_match_conf, bins=20, alpha=0.7, label='No Match', color='red')\n",
    "    axes[1, 0].set_xlabel('Confidence Score')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('Confidence Score Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Precision-Recall Curve\n",
    "    if len(y_true.unique()) > 1 and y_scores.sum() > 0:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "        axes[1, 1].plot(recall, precision, 'b-')\n",
    "        axes[1, 1].set_xlabel('Recall')\n",
    "        axes[1, 1].set_ylabel('Precision')\n",
    "        axes[1, 1].set_title('Precision-Recall Curve')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Insufficient data for PR curve',\n",
    "                        ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Precision-Recall Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    plot_metrics(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "# Note: Full grid can take a long time. Start with a smaller grid.\n",
    "\n",
    "PARAM_GRID_SMALL = {\n",
    "    'tolerance': [0.5, 0.6],\n",
    "    'model': ['hog'],\n",
    "    'encoding_model': ['large'],\n",
    "    'num_jitters': [10]\n",
    "}\n",
    "\n",
    "PARAM_GRID_FULL = {\n",
    "    'tolerance': [0.4, 0.5, 0.6, 0.7],\n",
    "    'model': ['hog', 'cnn'],\n",
    "    'encoding_model': ['small', 'large'],\n",
    "    'num_jitters': [1, 10, 50]\n",
    "}\n",
    "\n",
    "# Select which grid to use\n",
    "param_grid = PARAM_GRID_SMALL  # Change to PARAM_GRID_FULL for exhaustive search\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "print(f\"Total combinations to test: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def run_grid_search(test_df, reference_photos, param_grid):\n",
    "    \"\"\"Run grid search over parameter combinations.\"\"\"\n",
    "    if len(test_df) == 0:\n",
    "        print(\"No test data available for grid search.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    combinations = list(product(*param_values))\n",
    "    \n",
    "    print(f\"Running grid search with {len(combinations)} combinations...\")\n",
    "    \n",
    "    for i, combo in enumerate(combinations):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        tolerance = params.pop('tolerance')  # Tolerance is used during matching, not training\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{len(combinations)}] Testing: tolerance={tolerance}, {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Train model with current parameters\n",
    "            provider, num_faces, training_time = train_model(\n",
    "                reference_photos,\n",
    "                model=params['model'],\n",
    "                encoding_model=params['encoding_model'],\n",
    "                num_jitters=params['num_jitters']\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            eval_df = evaluate_model(provider, test_df, tolerance=tolerance)\n",
    "            metrics = calculate_metrics(eval_df)\n",
    "            \n",
    "            if metrics:\n",
    "                result = {\n",
    "                    'tolerance': tolerance,\n",
    "                    'model': params['model'],\n",
    "                    'encoding_model': params['encoding_model'],\n",
    "                    'num_jitters': params['num_jitters'],\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1_score': metrics['f1_score'],\n",
    "                    'auc': metrics['auc'],\n",
    "                    'training_time': training_time,\n",
    "                    'avg_inference_time': metrics['avg_processing_time']\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"   -> Accuracy: {metrics['accuracy']:.3f}, F1: {metrics['f1_score']:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   -> Error: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run grid search\n",
    "if len(test_df) > 0:\n",
    "    grid_results_df = run_grid_search(test_df, reference_photos, param_grid)\n",
    "    \n",
    "    if len(grid_results_df) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Grid Search Results (sorted by F1 Score):\")\n",
    "        print(\"=\"*60)\n",
    "        display(grid_results_df.sort_values('f1_score', ascending=False))\n",
    "else:\n",
    "    print(\"No test data. Please add test photos to run grid search.\")\n",
    "    grid_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "if len(grid_results_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: F1 Score by Tolerance\n",
    "    if 'tolerance' in grid_results_df.columns:\n",
    "        for model in grid_results_df['model'].unique():\n",
    "            subset = grid_results_df[grid_results_df['model'] == model]\n",
    "            axes[0].plot(subset['tolerance'], subset['f1_score'], 'o-', label=model)\n",
    "        axes[0].set_xlabel('Tolerance')\n",
    "        axes[0].set_ylabel('F1 Score')\n",
    "        axes[0].set_title('F1 Score vs Tolerance')\n",
    "        axes[0].legend()\n",
    "    \n",
    "    # Plot 2: Accuracy vs Training Time\n",
    "    scatter = axes[1].scatter(\n",
    "        grid_results_df['training_time'],\n",
    "        grid_results_df['accuracy'],\n",
    "        c=grid_results_df['f1_score'],\n",
    "        cmap='viridis',\n",
    "        s=100\n",
    "    )\n",
    "    plt.colorbar(scatter, ax=axes[1], label='F1 Score')\n",
    "    axes[1].set_xlabel('Training Time (s)')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Accuracy vs Training Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best parameters\n",
    "    if len(grid_results_df) > 0:\n",
    "        best = grid_results_df.loc[grid_results_df['f1_score'].idxmax()]\n",
    "        print(\"\\nBest Parameters (by F1 Score):\")\n",
    "        print(f\"  Tolerance: {best['tolerance']}\")\n",
    "        print(f\"  Model: {best['model']}\")\n",
    "        print(f\"  Encoding Model: {best['encoding_model']}\")\n",
    "        print(f\"  Num Jitters: {best['num_jitters']}\")\n",
    "        print(f\"  F1 Score: {best['f1_score']:.3f}\")\n",
    "        print(f\"  Accuracy: {best['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_DIR = Path(os.getcwd()) / \"experiments\"\n",
    "EXPERIMENTS_FILE = EXPERIMENTS_DIR / \"experiments.json\"\n",
    "\n",
    "def save_experiment(params, dataset_info, metrics, notes=\"\"):\n",
    "    \"\"\"Save an experiment to the experiments file.\"\"\"\n",
    "    EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load existing experiments\n",
    "    experiments = []\n",
    "    if EXPERIMENTS_FILE.exists():\n",
    "        with open(EXPERIMENTS_FILE, 'r') as f:\n",
    "            experiments = json.load(f)\n",
    "    \n",
    "    # Create new experiment\n",
    "    experiment = {\n",
    "        'id': f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'parameters': params,\n",
    "        'dataset': dataset_info,\n",
    "        'results': metrics,\n",
    "        'notes': notes\n",
    "    }\n",
    "    \n",
    "    experiments.append(experiment)\n",
    "    \n",
    "    # Save back\n",
    "    with open(EXPERIMENTS_FILE, 'w') as f:\n",
    "        json.dump(experiments, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Saved experiment: {experiment['id']}\")\n",
    "    return experiment\n",
    "\n",
    "def load_experiments():\n",
    "    \"\"\"Load all saved experiments.\"\"\"\n",
    "    if not EXPERIMENTS_FILE.exists():\n",
    "        return []\n",
    "    \n",
    "    with open(EXPERIMENTS_FILE, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def experiments_to_dataframe(experiments):\n",
    "    \"\"\"Convert experiments to a DataFrame for analysis.\"\"\"\n",
    "    rows = []\n",
    "    for exp in experiments:\n",
    "        row = {\n",
    "            'id': exp['id'],\n",
    "            'timestamp': exp['timestamp'],\n",
    "            'notes': exp.get('notes', '')\n",
    "        }\n",
    "        row.update(exp.get('parameters', {}))\n",
    "        row.update({f\"dataset_{k}\": v for k, v in exp.get('dataset', {}).items()})\n",
    "        row.update(exp.get('results', {}))\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(f\"Experiments will be saved to: {EXPERIMENTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current experiment (if we have results)\n",
    "if len(results_df) > 0 and metrics:\n",
    "    current_params = {\n",
    "        'tolerance': face_config.get('tolerance', 0.6),\n",
    "        'model': local_config.get('model', 'hog'),\n",
    "        'encoding_model': local_config.get('encoding_model', 'large'),\n",
    "        'num_jitters': local_config.get('training', {}).get('num_jitters', 50)\n",
    "    }\n",
    "    \n",
    "    dataset_info = {\n",
    "        'name': 'test_photos',\n",
    "        'match_count': len(test_df[test_df['ground_truth'] == 'match']),\n",
    "        'no_match_count': len(test_df[test_df['ground_truth'] == 'no_match']),\n",
    "        'total': len(test_df)\n",
    "    }\n",
    "    \n",
    "    experiment = save_experiment(\n",
    "        params=current_params,\n",
    "        dataset_info=dataset_info,\n",
    "        metrics=metrics,\n",
    "        notes=\"Initial experiment\"\n",
    "    )\n",
    "    print(f\"\\nExperiment saved with ID: {experiment['id']}\")\n",
    "else:\n",
    "    print(\"No results to save. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all experiments\n",
    "experiments = load_experiments()\n",
    "\n",
    "if experiments:\n",
    "    exp_df = experiments_to_dataframe(experiments)\n",
    "    print(f\"Loaded {len(experiments)} experiments:\\n\")\n",
    "    display(exp_df[['id', 'timestamp', 'tolerance', 'model', 'accuracy', 'f1_score', 'notes']].tail(10))\n",
    "else:\n",
    "    print(\"No experiments saved yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare experiments over time\n",
    "if experiments and len(experiments) > 1:\n",
    "    exp_df = experiments_to_dataframe(experiments)\n",
    "    exp_df['timestamp'] = pd.to_datetime(exp_df['timestamp'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Metrics over time\n",
    "    axes[0].plot(exp_df['timestamp'], exp_df['accuracy'], 'o-', label='Accuracy')\n",
    "    axes[0].plot(exp_df['timestamp'], exp_df['f1_score'], 's-', label='F1 Score')\n",
    "    if 'precision' in exp_df.columns:\n",
    "        axes[0].plot(exp_df['timestamp'], exp_df['precision'], '^-', label='Precision')\n",
    "    if 'recall' in exp_df.columns:\n",
    "        axes[0].plot(exp_df['timestamp'], exp_df['recall'], 'v-', label='Recall')\n",
    "    axes[0].set_xlabel('Timestamp')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('Metrics Over Time')\n",
    "    axes[0].legend()\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: F1 Score by Parameters\n",
    "    if 'tolerance' in exp_df.columns:\n",
    "        exp_df_sorted = exp_df.sort_values('f1_score', ascending=True)\n",
    "        labels = [f\"tol={row['tolerance']}, {row['model']}\" for _, row in exp_df_sorted.iterrows()]\n",
    "        axes[1].barh(range(len(labels)), exp_df_sorted['f1_score'])\n",
    "        axes[1].set_yticks(range(len(labels)))\n",
    "        axes[1].set_yticklabels(labels)\n",
    "        axes[1].set_xlabel('F1 Score')\n",
    "        axes[1].set_title('F1 Score by Configuration')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 experiments to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference: Parameter Guidelines\n",
    "\n",
    "| Parameter | Values | Recommendation |\n",
    "|-----------|--------|----------------|\n",
    "| `tolerance` | 0.4-0.7 | Start with 0.6, lower for stricter matching |\n",
    "| `model` | hog, cnn | Use `hog` for speed, `cnn` for accuracy (requires GPU) |\n",
    "| `encoding_model` | small, large | Use `large` for better accuracy |\n",
    "| `num_jitters` | 1-100 | Higher = more robust, but slower. Use 50 for training, 1 for inference |\n",
    "\n",
    "### Workflow\n",
    "1. Add reference photos to `reference_photos/`\n",
    "2. Add test photos to `notebooks/data/test_photos/match/` and `no_match/`\n",
    "3. Run training and evaluation\n",
    "4. Adjust parameters and re-run\n",
    "5. Save experiments to track progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
